# â­ Task 2 â€“ Distributed Data Processing (Using Apache Spark)

*COMPANY* : CODTECH IT SOLUTIONS

*NAME* : 

*INTERN ID* : 

*DOMAIN* : 

*DURATION*: 

*MENTOR* : 

In this task, Apache Spark was used to analyze a **large cleaned retail dataset** and perform distributed operations such as filtering, grouping, aggregation, and generating business insights at scale.

---

## âœ… 1. Objective of This Task

The goal of Task 2 was to:

- Use **Apache Spark** to analyze a large dataset  
- Apply **filtering**, **grouping**, and **aggregation** operations  
- Generate business insights such as:
  - Revenue by country  
  - Top customers  
  - Top products  
  - Monthly revenue trend  
- Save all analysis outputs as CSV files for visualization (Task 3)

This task demonstrates the use of Spark for real-world **Big Data analytical processing**.

---

## ğŸ“‚ 2. Dataset Used

Input dataset:

âœ” `cleaned_retail.csv` (generated from Task 1)

Contains **392,692 rows** after cleaning.  
Includes: InvoiceNo, StockCode, Description, Quantity, UnitPrice, CustomerID, Country, InvoiceDate, TotalPrice, etc.

---

## âš™ï¸ 3. Tools & Technologies

- **Python 3.11**  
- **Apache PySpark**  
- **VS Code**  
- **Spark DataFrame API**

---

## ğŸ” 4. Analysis Performed (Spark Data Processing)

The following distributed operations were performed step-by-step:

---

### **âœ” Step 1 â€” Initialize Spark Session**
Creates the Spark engine to process large data efficiently.

---

### **âœ” Step 2 â€” Load Cleaned Dataset**
```df = spark.read.csv("cleaned_retail.csv", header=True, inferSchema=True)```

Printed:
- Total rows  
- Total columns  
- Sample preview  

---

### **âœ” Step 3 â€” Filtering**
Examples:

#### a) Filter transactions from â€œUnited Kingdomâ€
```python```
```df.filter(df.Country == "United Kingdom")```
#### b) Filter high-value transactions:
```TotalPrice > 100```

## âœ” Step 4 â€” Grouping & Aggregations

### ğŸ”· Revenue by Country
Performed groupBy aggregation:
```groupBy("Country").agg(sum("TotalPrice"))```


Saved as: **revenue_by_country.csv**

---

### ğŸ”· Top 10 Products by Revenue
Grouped using **StockCode + Description**:

- `sum(TotalPrice)` â†’ Revenue  
- `sum(Quantity)` â†’ TotalQuantity  

Saved as: **top_10_products.csv**

---

### ğŸ”· Top 10 Customers
Grouped using **CustomerID**:

- `sum(TotalPrice)`  
- `sum(Quantity)`  

Saved as: **top_10_customers.csv**

---

### ğŸ”· Invoice-level Summary
Revenue per invoice was computed:

- **InvoiceRevenue = sum(TotalPrice)**
- **InvoiceQuantity = sum(Quantity)**

Saved as: **transactions_by_invoice.csv**

---

## ğŸ“Š 5. Key Summary Statistics Generated

A final KPI summary file was created:

âœ” **summary_total_revenue.csv**

Contains the following:

| Metric | Value |
|--------|--------|
| Total Revenue | **8.89M** |
| Total Unique Customers | **4,338** |
| Total Transactions | **18,532** |
| Avg Order Value | **479.56** |

This file is used in Power BI for the KPI cards in the Executive Summary (Task 3).

---

## ğŸ“… 6. Monthly Revenue Trend (Pandas + Spark)

Spark dataframe was converted to Pandas for date handling:
```YearMonth = InvoiceDate_parsed.dt.to_period("M")```


Revenue was summed month-wise and saved as:

âœ” **monthly_revenue.csv**

This file powers the **Monthly Revenue Trend Line Chart** in Task 3.

---

## ğŸ“ 7. Output Files Generated

All results were saved as CSV files:

| Output File | Description |
|-------------|-------------|
| revenue_by_country.csv | Revenue generated by each country |
| top_10_products.csv | Highest-selling products |
| top_10_customers.csv | Most valuable customers |
| transactions_by_invoice.csv | Revenue + quantity per invoice |
| summary_total_revenue.csv | KPI metrics for executive dashboard |
| monthly_revenue.csv | Monthly revenue trend |

These CSV files are used directly inside the Power BI dashboards for Task 3.

---

## ğŸ§ª 8. How to Run the Spark Script

Run the analysis file using:
```py task2_analysis.py```

Ensure the following files exist in the same folder:

- `task2_analysis.py`  
- `cleaned_retail.csv`

---

## ğŸ“˜ 9. Learning Outcomes

Through this task, the following Spark concepts were learned:

- Distributed data processing  
- Filtering and conditional queries  
- GroupBy and aggregation functions  
- Creating business KPIs at scale  
- Generating month-wise revenue trends  
- Saving distributed outputs for BI tools  

This task forms the analytical foundation for Task 3 (Data Visualization).

---



