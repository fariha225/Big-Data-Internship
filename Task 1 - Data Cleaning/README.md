Task 1 â€“ Big Data Cleaning (Using PySpark)

This task focuses on loading a large dataset using Apache Spark and performing essential data cleaning operations such as handling missing values, removing invalid records, converting data types, and generating a cleaned output file.

âœ… 1. Objective of This Task

The goal of Task 1 was to:
â¦	Load a large retail dataset using Apache Spark
â¦	Perform manual data handling & cleaning
â¦	Remove duplicates
â¦	Filter invalid/negative records
â¦	Clean text columns
â¦	Add new calculated fields
â¦	Save the cleaned output as a new file

This task demonstrates basic data engineering skills using Spark for big-data processing.


ğŸ“‚ 2. Dataset Used

Dataset Name: Online Retail Dataset (UCI Repository)
File: OnlineRetail.csv


Contains customer transactions from a UK-based retailer

Rows: 541,909
Columns: 8


Includes details like:

Invoice number
Product code
Description
Quantity
Invoice date
Price
Customer ID
Country


âš™ï¸ 3. Tools & Technologies

Python 3.11
Apache PySpark
VS Code for running scripts


ğŸ” 4. Data Cleaning Steps Performed

Below is the exact cleaning pipeline implemented in PySpark.

â¦	Step 1 â€” Start Spark Session

Â   Initialize Spark engine with required memory.

â¦	Step 2 â€” Load Raw CSV

Â   Read file with header
Â   Infer schema automatically
Â   Print row/column count + schema

â¦	Step 3 â€” Remove Cancellation Invoices

Â   Invoices starting with â€œCâ€ indicate returns.
Â   These were removed.

â¦	Step 4 â€” Remove Duplicate Rows

Â   Ensures cleaner and more accurate analysis.

â¦	Step 5 â€” Handle Missing Customer IDs

Â   Rows where CustomerID was null were removed.

â¦	Step 6 â€” Remove Invalid Quantity & Unit Price

Â   Removed rows where:

Â   Quantity â‰¤ 0
Â   Unit price â‰¤ 0

â¦	Step 7 â€” Clean Product Description

Â   Converted to lowercase

Â   Trimmed extra spaces

â¦	Step 9 â€” Add TotalPrice Column

Â   TotalPrice = Quantity Ã— UnitPrice

â¦	Step 10 â€” Save Cleaned Data

Â   Output saved as:
Â   cleaned_retail.csv


ğŸ“Š 5. Before vs After Cleaning
Metric	                 Before Cleaning     After Cleaning
Rows	                   541,909   	         392,692
Columns	                 8	                 10 (added InvoiceDateTS & TotalPrice)
Missing Customer IDs	   135,080	           Removed
Negative/Zero Quantity	 10,624	             Removed
Negative/Zero UnitPrice	 2,517	             Removed
Cancellation Invoices	   9,288	             Removed


ğŸ§¾ 6. Final Output

The cleaned dataset is exported as:

âœ” cleaned_retail.csv

This file is used in Task 2 (Distributed Processing).




ğŸ§ª 7. How to Run the Script

Open terminal and run:

py task1_cleaning.py


Make sure the following files are in the same folder:

task1_cleaning.py
OnlineRetail.csv



ğŸ“˜ 8. Learning Outcomes

â¦	How to process large datasets using Spark
â¦	How to apply filtering, deduplication, and missing-value handling
â¦	How to clean text fields
â¦	How to add calculated columns
â¦	How to export cleaned output

This task is foundational for the upcoming analysis and visualizations.
