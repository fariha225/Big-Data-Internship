# â­ Task 1 â€“ Big Data Cleaning (Using PySpark)

*COMPANY* : CODTECH IT SOLUTIONS

*NAME* : Syeda Fariha Fatima 

*INTERN ID* : CT04DR818

*DOMAIN* : Big Data

*DURATION*: 4 weeks

*MENTOR* : Neela Santhosh Kumar

This task focuses on cleaning a **large retail dataset** using **Apache Spark**, performing operations such as handling missing values, removing invalid records, converting data types, and generating a cleaned dataset for further analysis.

---

## âœ… 1. Objective of This Task

The goal of Task 1 was to:

- Load a large dataset using **Apache Spark**
- Perform **manual data handling & cleaning**
- Remove **duplicates**
- Filter invalid or negative records
- Clean text fields
- Add **new calculated columns**
- Save the **final cleaned dataset**

This task demonstrates core **data engineering skills** for Big Data processing.

---

## ğŸ“‚ 2. Dataset Used

**Dataset:** Online Retail Dataset (UCI Repository)  
**File:** `OnlineRetail.csv`

Contains customer transaction records from a UK-based retailer.

| Property | Value |
|---------|--------|
| **Rows** | 541,909 |
| **Columns** | 8 |
| **Includes** | InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country |

---

## âš™ï¸ 3. Tools & Technologies

- **Python 3.11**
- **Apache PySpark**
- **VS Code** (to run scripts)

---

## ğŸ” 4. Data Cleaning Steps Performed

Below is the complete cleaning pipeline implemented in PySpark:

### **Step 1 â€” Start Spark Session**
- Initialize Spark engine with memory configs.

### **Step 2 â€” Load Raw CSV**
- Load CSV with header  
- Infer schema  
- Print dataset size + schema  

### **Step 3 â€” Remove Cancellation Invoices**
- Invoices starting with **â€œCâ€** indicate returns â†’ removed.

### **Step 4 â€” Remove Duplicate Rows**
- Ensures correctness of further analysis.

### **Step 5 â€” Handle Missing Customer IDs**
- All rows with **CustomerID = NULL** removed.

### **Step 6 â€” Remove Invalid Quantity & UnitPrice**
Removed rows where:
- Quantity **â‰¤ 0**
- UnitPrice **â‰¤ 0**

### **Step 7 â€” Clean Description Field**
- Lowercased text  
- Trimmed whitespace

### **Step 8 â€” Convert Invoice Date to Timestamp**
- Added new column: `InvoiceDateTS`

### **Step 9 â€” Add TotalPrice Column**
- TotalPrice = Quantity Ã— UnitPrice

### **Step 10 â€” Save Final Cleaned Output**
- Output saved as: **cleaned_retail.csv**

---

## ğŸ“Š 5. Before vs After Cleaning

| Metric | Before Cleaning | After Cleaning |
|--------|------------------|----------------|
| **Rows** | 541,909 | 392,692 |
| **Columns** | 8 | 10 (added InvoiceDateTS & TotalPrice) |
| **Missing Customer IDs** | 135,080 | Removed |
| **Negative/Zero Quantity** | 10,624 | Removed |
| **Negative/Zero UnitPrice** | 2,517 | Removed |
| **Cancellation Invoices** | 9,288 | Removed |

---

## ğŸ§¾ 6. Final Output

The cleaned dataset generated:

âœ” `cleaned_retail.csv`  
(used in Task 2 â€“ Distributed Data Processing)

---

## ğŸ§ª 7. How to Run the Script

To run the cleaning script:

```py task1_cleaning.py ```

Make sure the following files are in the same folder:

task1_cleaning.py
OnlineRetail.csv

## ğŸ“˜ 8. Learning Outcomes

This task helped in understanding:

- Loading & processing large datasets using PySpark

- Handling missing values & invalid records

- Filtering, deduplication, and text cleaning

- Adding calculated columns

- Exporting cleaned datasets for further analysis


---


